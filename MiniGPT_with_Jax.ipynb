{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ZePpgW6jLvja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S5olYdwfKYI-"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pp\n",
        "import tqdm\n",
        "import unittest\n",
        "import time\n",
        "import functools\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhxprQ-fKq9o",
        "outputId": "1967d79c-c01a-4efc-9ab2-4af5fd77029d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-11 18:13:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-08-11 18:13:14 (17.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiddle\n",
        "\n",
        "\n",
        "import fiddle as fdl\n",
        "from fiddle import graphviz\n",
        "from fiddle.experimental import auto_config\n",
        "import fiddle.extensions.jax\n",
        "\n",
        "fiddle.extensions.jax.enable()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ugUn8rEkdDu",
        "outputId": "d0956cd4-ba09-4f3a-ac0e-1b223125a7d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fiddle\n",
            "  Downloading fiddle-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from fiddle) (1.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from fiddle) (0.20.3)\n",
            "Collecting libcst (from fiddle)\n",
            "  Downloading libcst-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from fiddle) (4.12.2)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.10/dist-packages (from libcst->fiddle) (6.0.2)\n",
            "Downloading fiddle-0.3.0-py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libcst-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libcst, fiddle\n",
            "Successfully installed fiddle-0.3.0 libcst-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions\n",
        "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
        "\n",
        "def get_batch(random_key, data, batch_size, block_size):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  ix = jax.random.randint(random_key, shape=(batch_size, 1), minval=0, maxval=len(data)-block_size)\n",
        "  x = dynamic_slice_vmap(data, ix, (block_size,))\n",
        "  y = dynamic_slice_vmap(data, ix+1, (block_size,))\n",
        "  return x, y\n",
        "\n",
        "def load_shakespeare_dataset():\n",
        "  with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "  data = jnp.array(encode(text))\n",
        "  n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "  train_data = data[:n]\n",
        "  eval_data = data[n:]\n",
        "  return train_data, eval_data\n",
        "\n",
        "def init_train_state(\n",
        "    model,\n",
        "    params,\n",
        "    learning_rate=1e-4,\n",
        "):\n",
        "  tx = optax.adam(learning_rate)\n",
        "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, x, y):\n",
        "  def _loss(params):\n",
        "    predictions = state.apply_fn(params, x, training=True)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(predictions, y)\n",
        "    return loss.mean()\n",
        "  loss, grads = jax.value_and_grad(_loss)(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, loss\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(state, x, y):\n",
        "  predictions = state.apply_fn(state.params, x, training=False)\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(predictions, y).mean()\n",
        "\n",
        "def run_training_loop(\n",
        "    num_iterations,\n",
        "    batch_size,\n",
        "    block_size,\n",
        "    learning_rate,\n",
        "    eval_data,\n",
        "    train_data,\n",
        "    model,\n",
        "):\n",
        "  \"\"\"\n",
        "  Runs the training loop for the specified model.\n",
        "\n",
        "  Args:\n",
        "      num_iterations (int): The number of training iterations.\n",
        "      batch_size (int): The number of samples in each batch.\n",
        "      block_size (int): The size of each block (sequence length).\n",
        "      learning_rate (float): The learning rate for the optimizer.\n",
        "      eval_data (array-like): 1d JAX array of integer tokens, consisting of evaluation data.\n",
        "      train_data (array-like): 1d JAX array of integer tokens, consisting of training data.\n",
        "      model (nn.Module, optional): A Jax Model object.\n",
        "\n",
        "  Returns:\n",
        "      state: The training state with the best eval metrics.\n",
        "\n",
        "  Example:\n",
        "      >>> final_state = run_training_loop(\n",
        "      >>>     num_iterations=1000,\n",
        "      >>>     batch_size=16,\n",
        "      >>>     block_size=32,\n",
        "      >>>     learning_rate=0.001,\n",
        "      >>>     eval_data=eval_data,\n",
        "      >>>     train_data=train_data,\n",
        "      >>>     model=mini_gpt\n",
        "      >>> )\n",
        "  \"\"\"\n",
        "  random_key = jax.random.PRNGKey(0)\n",
        "  x = jnp.ones((batch_size, block_size), dtype=jnp.int16)\n",
        "  random_key, random_subkey = jax.random.split(random_key)\n",
        "  params = model.init(random_subkey, x, training=False)\n",
        "  preds = model.apply(params, x)\n",
        "  state = init_train_state(\n",
        "      model, params, learning_rate=learning_rate)\n",
        "  predictions = state.apply_fn(state.params, x, training=True)\n",
        "  best_state = state\n",
        "  best_eval_loss = math.inf\n",
        "  for i in range(num_iterations):\n",
        "    random_key, random_subkey = jax.random.split(random_key)\n",
        "    x, y = get_batch(random_subkey, train_data, batch_size=batch_size, block_size=block_size)\n",
        "    state, loss = train_step(state, x, y)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      eval_loss = eval_step(state, *get_batch(random_subkey, eval_data, batch_size=batch_size, block_size=block_size))\n",
        "      print(f\"Step: {i}\\t train loss: {loss}\\t eval loss: {eval_loss}\")\n",
        "      if eval_loss < best_eval_loss:\n",
        "        best_eval_loss = eval_loss\n",
        "        best_state = state\n",
        "  return best_state"
      ],
      "metadata": {
        "id": "jIoEU_PAkk9T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and tokenize dataset"
      ],
      "metadata": {
        "id": "7bhOfS-TL0iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqg4qFTxK3W0",
        "outputId": "63e76d17-2ffe-4fb9-c451-e077c2de9074"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "data = jnp.array(encode(text))\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "eval_data = data[n:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-fbEt4DLNGp",
        "outputId": "4ce63753-9c15-415c-fcbd-0256f2e7d085"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A simple text decoder model\n",
        "\n",
        "The SimpleDecoder below will predict the next token given a single token."
      ],
      "metadata": {
        "id": "qu4if-0aLfAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDecoder(nn.Module):\n",
        "  vocab_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.vocab_size)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    return self.token_embedding(x) # B, T, vocab_size\n",
        "\n",
        "  def generate(self, start_token, max_length=20, end_token=None):\n",
        "    # Initialize the generated sequence with the start token\n",
        "    generated_sequence = [start_token]\n",
        "    current_token = start_token\n",
        "\n",
        "    for _ in range(max_length - 1):  # We already have the start token\n",
        "      # Convert the current token to a tensor\n",
        "      current_token_tensor = jnp.array([[current_token]])\n",
        "\n",
        "      # Get the token embeddings\n",
        "      token_logits = self.__call__(current_token_tensor)\n",
        "\n",
        "      # Get the token with the highest probability\n",
        "      next_token = jnp.argmax(token_logits, axis=-1)[0]\n",
        "\n",
        "      # Append the next token to the generated sequence\n",
        "      generated_sequence.append(int(next_token[0]))\n",
        "\n",
        "      # If the end token is generated, stop the generation\n",
        "      if end_token is not None and next_token[0] == end_token:\n",
        "          break\n",
        "\n",
        "      # Update the current token\n",
        "      current_token = int(next_token[0])\n",
        "\n",
        "    return generated_sequence\n",
        "\n",
        "decoder = SimpleDecoder(vocab_size=vocab_size)\n",
        "start_token = 23\n",
        "dummy = jnp.ones((4, 8), dtype=jnp.int16)\n",
        "params = decoder.init(jax.random.PRNGKey(0), dummy)\n",
        "\n",
        "# Generate a sequence\n",
        "generated_sequence = decoder.apply(params, start_token, method=decoder.generate, max_length=20)\n",
        "print(\"Generated sequence:\", decode(generated_sequence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KYMAEy19OUsq",
        "outputId": "bc5b5233-3d48-4224-d00c-5021a0b5078d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sequence: KplEzUplEzUplEzUplEz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Generated sequence is gibberish. Let's see if it gets better when we train it."
      ],
      "metadata": {
        "id": "sl79uLx4QXd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can play around the parameters here to see how that affects loss.\n",
        "num_iterations = 7000\n",
        "learning_rate = 1e-3\n",
        "num_layers = 4\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "num_heads = 4\n",
        "hidden_dim = 64\n",
        "\n",
        "decoder = SimpleDecoder(vocab_size=vocab_size)\n",
        "\n",
        "simple_decoder_state = run_training_loop(\n",
        "    num_iterations = num_iterations,\n",
        "    learning_rate = learning_rate,\n",
        "    batch_size = batch_size,\n",
        "    block_size = block_size,\n",
        "    eval_data = eval_data,\n",
        "    train_data = train_data,\n",
        "    model = decoder\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zyGvbsMHQrwF",
        "outputId": "68a94daa-1831-416c-aa52-d2b481237737"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0\t train loss: 4.199781894683838\t eval loss: 4.198739051818848\n",
            "Step: 100\t train loss: 4.068894386291504\t eval loss: 4.066617488861084\n",
            "Step: 200\t train loss: 3.953348159790039\t eval loss: 3.9613149166107178\n",
            "Step: 300\t train loss: 3.8457019329071045\t eval loss: 3.844743490219116\n",
            "Step: 400\t train loss: 3.751326084136963\t eval loss: 3.749478578567505\n",
            "Step: 500\t train loss: 3.6622512340545654\t eval loss: 3.6663739681243896\n",
            "Step: 600\t train loss: 3.576357126235962\t eval loss: 3.5837724208831787\n",
            "Step: 700\t train loss: 3.4955177307128906\t eval loss: 3.4985411167144775\n",
            "Step: 800\t train loss: 3.428492307662964\t eval loss: 3.4187421798706055\n",
            "Step: 900\t train loss: 3.3509371280670166\t eval loss: 3.306957721710205\n",
            "Step: 1000\t train loss: 3.289257526397705\t eval loss: 3.2913177013397217\n",
            "Step: 1100\t train loss: 3.1995325088500977\t eval loss: 3.2193007469177246\n",
            "Step: 1200\t train loss: 3.139836549758911\t eval loss: 3.1519577503204346\n",
            "Step: 1300\t train loss: 3.0998642444610596\t eval loss: 3.1109230518341064\n",
            "Step: 1400\t train loss: 3.0533289909362793\t eval loss: 3.036480188369751\n",
            "Step: 1500\t train loss: 2.966494083404541\t eval loss: 3.030930280685425\n",
            "Step: 1600\t train loss: 3.0061469078063965\t eval loss: 2.942122220993042\n",
            "Step: 1700\t train loss: 2.865622043609619\t eval loss: 2.865171432495117\n",
            "Step: 1800\t train loss: 2.8829379081726074\t eval loss: 2.891833782196045\n",
            "Step: 1900\t train loss: 2.8112235069274902\t eval loss: 2.8484928607940674\n",
            "Step: 2000\t train loss: 2.738637685775757\t eval loss: 2.8405308723449707\n",
            "Step: 2100\t train loss: 2.751099109649658\t eval loss: 2.7675931453704834\n",
            "Step: 2200\t train loss: 2.7462127208709717\t eval loss: 2.761674165725708\n",
            "Step: 2300\t train loss: 2.7178125381469727\t eval loss: 2.7805278301239014\n",
            "Step: 2400\t train loss: 2.757195472717285\t eval loss: 2.6995322704315186\n",
            "Step: 2500\t train loss: 2.689034938812256\t eval loss: 2.7554397583007812\n",
            "Step: 2600\t train loss: 2.7382900714874268\t eval loss: 2.6596200466156006\n",
            "Step: 2700\t train loss: 2.612903356552124\t eval loss: 2.7050375938415527\n",
            "Step: 2800\t train loss: 2.602550983428955\t eval loss: 2.7208943367004395\n",
            "Step: 2900\t train loss: 2.639296770095825\t eval loss: 2.5894811153411865\n",
            "Step: 3000\t train loss: 2.623594284057617\t eval loss: 2.642141580581665\n",
            "Step: 3100\t train loss: 2.665217638015747\t eval loss: 2.5994555950164795\n",
            "Step: 3200\t train loss: 2.656813383102417\t eval loss: 2.5974864959716797\n",
            "Step: 3300\t train loss: 2.615386962890625\t eval loss: 2.590294122695923\n",
            "Step: 3400\t train loss: 2.520057439804077\t eval loss: 2.5909483432769775\n",
            "Step: 3500\t train loss: 2.53633451461792\t eval loss: 2.4745278358459473\n",
            "Step: 3600\t train loss: 2.5341784954071045\t eval loss: 2.590409755706787\n",
            "Step: 3700\t train loss: 2.541506767272949\t eval loss: 2.621236562728882\n",
            "Step: 3800\t train loss: 2.5514590740203857\t eval loss: 2.5236399173736572\n",
            "Step: 3900\t train loss: 2.6021013259887695\t eval loss: 2.5444014072418213\n",
            "Step: 4000\t train loss: 2.528709888458252\t eval loss: 2.524250030517578\n",
            "Step: 4100\t train loss: 2.5516016483306885\t eval loss: 2.565639019012451\n",
            "Step: 4200\t train loss: 2.4526093006134033\t eval loss: 2.5241329669952393\n",
            "Step: 4300\t train loss: 2.5453267097473145\t eval loss: 2.5637640953063965\n",
            "Step: 4400\t train loss: 2.4965860843658447\t eval loss: 2.474672317504883\n",
            "Step: 4500\t train loss: 2.475438356399536\t eval loss: 2.6402475833892822\n",
            "Step: 4600\t train loss: 2.501800537109375\t eval loss: 2.470865488052368\n",
            "Step: 4700\t train loss: 2.4368417263031006\t eval loss: 2.490649461746216\n",
            "Step: 4800\t train loss: 2.4575552940368652\t eval loss: 2.5727343559265137\n",
            "Step: 4900\t train loss: 2.4671266078948975\t eval loss: 2.5449554920196533\n",
            "Step: 5000\t train loss: 2.53952693939209\t eval loss: 2.5675764083862305\n",
            "Step: 5100\t train loss: 2.5107016563415527\t eval loss: 2.493584156036377\n",
            "Step: 5200\t train loss: 2.505016326904297\t eval loss: 2.4732346534729004\n",
            "Step: 5300\t train loss: 2.60979962348938\t eval loss: 2.5939736366271973\n",
            "Step: 5400\t train loss: 2.527073383331299\t eval loss: 2.466932773590088\n",
            "Step: 5500\t train loss: 2.4361040592193604\t eval loss: 2.4668498039245605\n",
            "Step: 5600\t train loss: 2.42936372756958\t eval loss: 2.537674903869629\n",
            "Step: 5700\t train loss: 2.4548511505126953\t eval loss: 2.4831881523132324\n",
            "Step: 5800\t train loss: 2.446113109588623\t eval loss: 2.455986738204956\n",
            "Step: 5900\t train loss: 2.536184787750244\t eval loss: 2.4467344284057617\n",
            "Step: 6000\t train loss: 2.443162202835083\t eval loss: 2.4697961807250977\n",
            "Step: 6100\t train loss: 2.4506020545959473\t eval loss: 2.5126099586486816\n",
            "Step: 6200\t train loss: 2.553802728652954\t eval loss: 2.5383307933807373\n",
            "Step: 6300\t train loss: 2.521543264389038\t eval loss: 2.5368001461029053\n",
            "Step: 6400\t train loss: 2.513913154602051\t eval loss: 2.4807417392730713\n",
            "Step: 6500\t train loss: 2.5263113975524902\t eval loss: 2.503016948699951\n",
            "Step: 6600\t train loss: 2.4243202209472656\t eval loss: 2.3928165435791016\n",
            "Step: 6700\t train loss: 2.449862480163574\t eval loss: 2.503744125366211\n",
            "Step: 6800\t train loss: 2.47369122505188\t eval loss: 2.4428603649139404\n",
            "Step: 6900\t train loss: 2.369575023651123\t eval loss: 2.469062566757202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sequence = decoder.apply(simple_decoder_state.params, start_token, method=decoder.generate, max_length=20)\n",
        "print(\"Generated sequence:\", decode(generated_sequence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xE_VcrDJHBmz",
        "outputId": "bb19150e-ed6f-4d56-b5f2-387d3c036fb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sequence: KI the the the the t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Implement MiniGPT.\n",
        "\n",
        "* You can use off-the-shelf Flax modules like Dense, LayerNorm. You may not use Flax's SelfAttention. Instead, use AttentionTask1 provided below.\n",
        "* Note that block_size, T, input context window length are different ways to refer to the same thing."
      ],
      "metadata": {
        "id": "5jhrHj0xLB_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B == batch_size.\n",
        "# T == number of tokens in sequence.\n",
        "# C == hidden_dim == hidden dimension of transformer.\n",
        "# head_dim == Head dimension for each Attention head. head_dim * num_heads == C.\n",
        "\n",
        "# You can use this class for solving Task 1. We will revisit this class in Task 2.\n",
        "class AttentionTask1(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.attention_impl = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=1, qkv_features=self.head_dim, dropout_rate=0.)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # x is of shape B, T, C.\n",
        "    q = self.query(x)  # B, T, head_dim\n",
        "    k = self.key(x)  # B, T, head_dim\n",
        "    v = self.value(x)  # B, T, head_dim\n",
        "    return self.attention_impl(inputs_q=q, inputs_k=k, inputs_v=v)  # B, T, head_dim\n",
        "\n",
        "# FeedForward is given to you for free.\n",
        "class FeedForward(nn.Module):\n",
        "  hidden_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.f1 = nn.Dense(features=4 * self.hidden_dim)\n",
        "    self.f2 = nn.Dense(features=self.hidden_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.f2(nn.relu(self.f1(x)))  # B, T, hidden_dim\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [AttentionTask1(self.head_dim) for _ in range(self.num_heads)]\n",
        "    self.dense = nn.Dense(self.num_heads*self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # TODO: Implement multi-head attention.\n",
        "    return self.dense(x)  # B, T, hidden_dim\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  hidden_dim: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    # head_dim * num_heads == hidden_dim should always hold true.\n",
        "    head_dim = self.hidden_dim // self.num_heads\n",
        "    # TODO: Fill out the rest of setup function.\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # TODO: Implement this function.\n",
        "    return x  # B, T, hidden_dim\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "  vocab_size: int\n",
        "  hidden_dim: int\n",
        "  block_size: int\n",
        "  num_layers: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.hidden_dim)\n",
        "    self.position_encoding = nn.Embed(\n",
        "        num_embeddings=self.block_size,\n",
        "        features=self.hidden_dim\n",
        "    )\n",
        "    self.final_dense = nn.Dense(features=self.vocab_size)\n",
        "    # TODO: Fill out the rest of this function.\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.token_embedding(x)  # B, T, hidden_dim\n",
        "\n",
        "    # TODO: Fill in missing functionalities here.\n",
        "\n",
        "    return self.final_dense(x)\n",
        "\n",
        "  def generate(self, random_key, params, x, max_new_tokens=50):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits = self.apply(params, x[:, -self.block_size:], training=False)\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      new_token = jax.random.categorical(random_subkey, logits[:, -1, :], axis=-1, shape=None)\n",
        "      x = jnp.concatenate([x, new_token[:, None]], axis=1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lsjrh9ZhOgzh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can play around the parameters here to see how that affects loss.\n",
        "num_iterations = 3000\n",
        "learning_rate = 1e-3\n",
        "num_layers = 4\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "num_heads = 4\n",
        "hidden_dim = 64\n",
        "\n",
        "mini_gpt = MiniGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_dim=hidden_dim,\n",
        "    block_size=block_size,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads\n",
        ")\n",
        "\n",
        "mini_gpt_state = run_training_loop(\n",
        "    num_iterations=num_iterations,\n",
        "    learning_rate=learning_rate,\n",
        "    batch_size=batch_size,\n",
        "    block_size=block_size,\n",
        "    eval_data=eval_data,\n",
        "    train_data=train_data,\n",
        "    model=mini_gpt\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "emkzC15clmUp",
        "outputId": "6f377dc1-ade1-4c31-dc3c-306ac434d8ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0\t train loss: 4.173918724060059\t eval loss: 4.173076629638672\n",
            "Step: 100\t train loss: 3.4989469051361084\t eval loss: 3.5021309852600098\n",
            "Step: 200\t train loss: 2.8072452545166016\t eval loss: 2.9120662212371826\n",
            "Step: 300\t train loss: 2.522160291671753\t eval loss: 2.640925884246826\n",
            "Step: 400\t train loss: 2.5444767475128174\t eval loss: 2.564183235168457\n",
            "Step: 500\t train loss: 2.5533204078674316\t eval loss: 2.5832293033599854\n",
            "Step: 600\t train loss: 2.5364367961883545\t eval loss: 2.5505051612854004\n",
            "Step: 700\t train loss: 2.571192979812622\t eval loss: 2.6175718307495117\n",
            "Step: 800\t train loss: 2.578876256942749\t eval loss: 2.5391509532928467\n",
            "Step: 900\t train loss: 2.5329501628875732\t eval loss: 2.427901029586792\n",
            "Step: 1000\t train loss: 2.6030311584472656\t eval loss: 2.5649704933166504\n",
            "Step: 1100\t train loss: 2.4884674549102783\t eval loss: 2.498779773712158\n",
            "Step: 1200\t train loss: 2.529825210571289\t eval loss: 2.5384504795074463\n",
            "Step: 1300\t train loss: 2.530698299407959\t eval loss: 2.529590606689453\n",
            "Step: 1400\t train loss: 2.5099973678588867\t eval loss: 2.4961094856262207\n",
            "Step: 1500\t train loss: 2.4723353385925293\t eval loss: 2.5274808406829834\n",
            "Step: 1600\t train loss: 2.5921216011047363\t eval loss: 2.4917685985565186\n",
            "Step: 1700\t train loss: 2.4279630184173584\t eval loss: 2.4016308784484863\n",
            "Step: 1800\t train loss: 2.509815216064453\t eval loss: 2.512701988220215\n",
            "Step: 1900\t train loss: 2.4338624477386475\t eval loss: 2.497684955596924\n",
            "Step: 2000\t train loss: 2.3769326210021973\t eval loss: 2.5317723751068115\n",
            "Step: 2100\t train loss: 2.4252068996429443\t eval loss: 2.463107109069824\n",
            "Step: 2200\t train loss: 2.4406867027282715\t eval loss: 2.4885549545288086\n",
            "Step: 2300\t train loss: 2.4652609825134277\t eval loss: 2.552493095397949\n",
            "Step: 2400\t train loss: 2.5351028442382812\t eval loss: 2.4718942642211914\n",
            "Step: 2500\t train loss: 2.474043369293213\t eval loss: 2.5576107501983643\n",
            "Step: 2600\t train loss: 2.549192428588867\t eval loss: 2.4582228660583496\n",
            "Step: 2700\t train loss: 2.426635265350342\t eval loss: 2.5263960361480713\n",
            "Step: 2800\t train loss: 2.4184322357177734\t eval loss: 2.5673046112060547\n",
            "Step: 2900\t train loss: 2.5119144916534424\t eval loss: 2.4508512020111084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment below to print predictions:\n",
        "# x = jnp.zeros((1, 1), dtype=jnp.int32)\n",
        "# random_key = jax.random.PRNGKey(0)\n",
        "# tokens = mini_gpt.generate(random_key, params=mini_gpt_state.params, x=x)\n",
        "# print(decode(tokens[0].tolist()))"
      ],
      "metadata": {
        "id": "gyB7K_wM6rXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass this test before moving on to Task 2.\n",
        "class TestTask1(unittest.TestCase):\n",
        "\n",
        "  def test_minigpt(self):\n",
        "    num_iterations = 4000\n",
        "    learning_rate = 1e-3\n",
        "    num_layers = 4\n",
        "    batch_size = 16\n",
        "    block_size = 32\n",
        "    num_heads = 4\n",
        "    hidden_dim = 64\n",
        "    random_key = jax.random.PRNGKey(42)\n",
        "\n",
        "    mini_gpt = MiniGPT(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_dim=hidden_dim,\n",
        "        block_size=block_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "\n",
        "    train_data, eval_data = load_shakespeare_dataset()\n",
        "    mini_gpt_state = run_training_loop(\n",
        "        num_iterations = num_iterations,\n",
        "        learning_rate = learning_rate,\n",
        "        batch_size = batch_size,\n",
        "        block_size = block_size,\n",
        "        eval_data = eval_data,\n",
        "        train_data = train_data,\n",
        "        model = mini_gpt\n",
        "    )\n",
        "    eval_losses = []\n",
        "    for _ in tqdm.tqdm(range(100)):\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      x, y = get_batch(\n",
        "          random_subkey, eval_data, batch_size=batch_size, block_size=block_size)\n",
        "      batch_eval_loss = eval_step(mini_gpt_state, x, y)\n",
        "      eval_losses.append(batch_eval_loss)\n",
        "    print(f\"Average eval loss: {np.mean(eval_losses)}\")\n",
        "    self.assertTrue(np.mean(eval_losses) < 1.9)\n",
        "\n",
        "# Uncomment the test below.\n",
        "# TestTask1().test_minigpt()"
      ],
      "metadata": {
        "id": "cleRdSmmvDSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - implement the Self-Attention Jax Module\n",
        "\n",
        "Your task is to implement Attention without using Flax's built-in nn.MultiHeadDotProductAttention module. Fill in the TODO section below.\n",
        "\n",
        "Things to keep in mind:\n",
        "\n",
        "* We are implementing a decoder-only transformer. This means that each token can only attend to previous tokens, but not future tokens."
      ],
      "metadata": {
        "id": "aezQPKA3OXLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionTask2(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    # Don't change the setup function.\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    # TODO: This function contains an incorrect attention implmentation. Change\n",
        "    # its definition below:\n",
        "    B, T, C = x.shape\n",
        "    return self.query(x) + self.key(x) + self.value(x) # B, T, head_dim"
      ],
      "metadata": {
        "id": "r0aff872OvIw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestAttention(unittest.TestCase):\n",
        "\n",
        "  EXPECTED_ATTENTION_ARRAY = np.array([[\n",
        "      [-2.3660736,  -1.0994253,   0.54647386,  1.663486,    1.0262686,\n",
        "      0.50164324, -0.40740347, -0.86529493,  1.6112939,  -0.46789974,\n",
        "      1.3150474,   0.9799258,  -0.5418715,  -1.2731858,  -0.7926506,\n",
        "      -0.8737542],\n",
        "      [-1.2626604,   1.3287369,   0.96550566,  0.4553011,   0.6900299,\n",
        "      -0.6283262,  -0.44400188,  0.18089633, -0.6977915,  -0.49270085,\n",
        "      0.1377207,   0.19912332, -0.02095406, -1.0335875,  -0.13449836,\n",
        "      -0.9766264],\n",
        "      [-2.1633344,  -0.7197231,   0.59619266,  1.4519494,   0.9575919,\n",
        "      0.33423916, -0.39966965, -0.69272554,  1.2452503,  -0.46386948,\n",
        "      1.1163911,   0.84217906, -0.45448378, -1.2067673,  -0.6738551,\n",
        "      -0.87238634],\n",
        "      [ 0.21197765,  0.2127537,  -0.27920845, -0.4683921,  -0.22381224,\n",
        "      0.49012795,  0.44253582,  0.2606917,   0.03008281,  0.06132472,\n",
        "      -0.28707987, -0.4550119,   0.16932811,  0.7396863,   0.54958737,\n",
        "      0.23469326]\n",
        "  ]])\n",
        "\n",
        "  def test_attention(self):\n",
        "    attention = AttentionTask2(head_dim=16)\n",
        "    key = jax.random.PRNGKey(0)\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params = attention.init(subkey, jnp.ones((1, 4, 8)))\n",
        "    x = jax.random.normal(key=key, shape=(1, 4, 8), dtype=jnp.float32)\n",
        "    y = attention.apply(params, x)\n",
        "    self.assertTrue(np.allclose(y, self.EXPECTED_ATTENTION_ARRAY))\n",
        "\n",
        "# Uncomment the test below.\n",
        "TestAttention().test_attention()"
      ],
      "metadata": {
        "id": "C3WxOMkcaV4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 Speed up MultiheadAttention with Einsum.\n",
        "\n",
        "Please finish task 2 first before doing this task."
      ],
      "metadata": {
        "id": "uAWkbRLV7gp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionTask3(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.dense = nn.Dense(features=self.num_heads * self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    # TODO: Implement this using Einsum.\n",
        "\n",
        "    return self.query(x) + self.key(x) + self.value(x) # B, T, head_dim\n"
      ],
      "metadata": {
        "id": "oAktMykm5TMe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestMultiHeadEinsum(unittest.TestCase):\n",
        "  EXPECTED_ATTENTION_ARRAY = np.array([[\n",
        "      [-2.3660736,  -1.0994253,   0.54647386,  1.663486,    1.0262686,\n",
        "      0.50164324, -0.40740347, -0.86529493,  1.6112939,  -0.46789974,\n",
        "      1.3150474,   0.9799258,  -0.5418715,  -1.2731858,  -0.7926506,\n",
        "      -0.8737542],\n",
        "      [-1.2626604,   1.3287369,   0.96550566,  0.4553011,   0.6900299,\n",
        "      -0.6283262,  -0.44400188,  0.18089633, -0.6977915,  -0.49270085,\n",
        "      0.1377207,   0.19912332, -0.02095406, -1.0335875,  -0.13449836,\n",
        "      -0.9766264],\n",
        "      [-2.1633344,  -0.7197231,   0.59619266,  1.4519494,   0.9575919,\n",
        "      0.33423916, -0.39966965, -0.69272554,  1.2452503,  -0.46386948,\n",
        "      1.1163911,   0.84217906, -0.45448378, -1.2067673,  -0.6738551,\n",
        "      -0.87238634],\n",
        "      [ 0.21197765,  0.2127537,  -0.27920845, -0.4683921,  -0.22381224,\n",
        "      0.49012795,  0.44253582,  0.2606917,   0.03008281,  0.06132472,\n",
        "      -0.28707987, -0.4550119,   0.16932811,  0.7396863,   0.54958737,\n",
        "      0.23469326]\n",
        "  ]])\n",
        "\n",
        "  def test_multihead_einsum(self):\n",
        "    attention_einsum = MultiHeadAttentionTask3(num_heads=2, head_dim=8)\n",
        "    key = jax.random.PRNGKey(0)\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params = attention_einsum.init(subkey, jnp.ones((1, 4, 16)))\n",
        "    x = jax.random.normal(key=key, shape=(1, 4, 16), dtype=jnp.float32)\n",
        "    y = attention_einsum.apply(params, x)\n",
        "    self.assertTrue(np.allclose(y, self.EXPECTED_ATTENTION_ARRAY))\n",
        "\n",
        "# TestMultiHeadEinsum().test_multihead_einsum()"
      ],
      "metadata": {
        "id": "bApnTKK-MQhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "* Run the Import section at the beginning of this colab before running the Solution.\n",
        "* The Solutions below need to be executed sequentially."
      ],
      "metadata": {
        "id": "i1RfdW6UpM9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution for task 1.\n",
        "class ReferenceAttention(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.attention_impl = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=1, qkv_features=self.head_dim, dropout_rate=0.)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x)  # B, T, head_dim\n",
        "    k = self.key(x)  # B, T, head_dim\n",
        "    v = self.value(x)  # B, T, head_dim\n",
        "    mask = jnp.tril(jnp.ones((B, 1, T, T)))\n",
        "    return self.attention_impl(inputs_q=q, inputs_k=k, inputs_v=v, mask=mask)  # B, T, head_dim\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  hidden_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.f1 = nn.Dense(features=4 * self.hidden_dim)\n",
        "    self.f2 = nn.Dense(features=self.hidden_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.f2(nn.relu(self.f1(x)))  # B, T, hidden_dim\n",
        "\n",
        "class MultiHeadAttentionSolution(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [ReferenceAttention(self.head_dim) for _ in range(self.num_heads)]\n",
        "    self.dense = nn.Dense(self.num_heads*self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n",
        "    return self.dense(x)  # B, T, hidden_dim\n",
        "\n",
        "class DecoderBlockSolution(nn.Module):\n",
        "  hidden_dim: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    head_dim = self.hidden_dim // self.num_heads\n",
        "    self.mha = MultiHeadAttentionSolution(\n",
        "        num_heads=self.num_heads,\n",
        "        head_dim=head_dim)\n",
        "    self.ff = FeedForward(self.hidden_dim)\n",
        "    self.ff_norm = nn.LayerNorm()\n",
        "    self.mha_norm = nn.LayerNorm()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = x + self.mha(self.mha_norm(x))\n",
        "    return x + self.ff(self.ff_norm(x))\n",
        "\n",
        "\n",
        "class MiniGPTSolution(nn.Module):\n",
        "  vocab_size: int\n",
        "  hidden_dim: int\n",
        "  block_size: int\n",
        "  num_layers: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.hidden_dim)\n",
        "    self.position_encoding = nn.Embed(\n",
        "        num_embeddings=self.block_size,\n",
        "        features=self.hidden_dim\n",
        "    )\n",
        "    self.decoder_blocks = [\n",
        "        DecoderBlockSolution(self.hidden_dim, self.num_heads) for _ in range(self.num_layers)\n",
        "    ]\n",
        "    self.final_norm = nn.LayerNorm()\n",
        "    self.final_dense = nn.Dense(features=self.vocab_size)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.token_embedding(x)  # B, T, hidden_dim\n",
        "    pos = self.position_encoding(jnp.arange(T))  # T, hidden_dim\n",
        "    x += pos\n",
        "    for block in self.decoder_blocks:\n",
        "      x = block(x)\n",
        "    return self.final_dense(self.final_norm(x))\n",
        "\n",
        "  def generate(self, random_key, params, x, max_new_tokens=50):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits = self.apply(params, x[:, -self.block_size:])\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      new_token = jax.random.categorical(random_subkey, logits[:, -1, :], axis=-1, shape=None)\n",
        "      x = jnp.concatenate([x, new_token[:, None]], axis=1)\n",
        "    return x\n",
        "\n",
        "# This is a duplicate of TestTask1.\n",
        "class TestTask1Solution(unittest.TestCase):\n",
        "\n",
        "  def test_minigpt(self):\n",
        "    num_iterations = 4000\n",
        "    learning_rate = 1e-3\n",
        "    num_layers = 4\n",
        "    batch_size = 16\n",
        "    block_size = 32\n",
        "    num_heads = 4\n",
        "    hidden_dim = 64\n",
        "    random_key = jax.random.PRNGKey(42)\n",
        "\n",
        "    model = MiniGPTSolution(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_dim=hidden_dim,\n",
        "        block_size=block_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "    train_data, eval_data = load_shakespeare_dataset()\n",
        "    mini_gpt_state = run_training_loop(\n",
        "        num_iterations = num_iterations,\n",
        "        learning_rate = learning_rate,\n",
        "        batch_size = batch_size,\n",
        "        block_size = block_size,\n",
        "        eval_data = eval_data,\n",
        "        train_data = train_data,\n",
        "        model = model\n",
        "    )\n",
        "    eval_losses = []\n",
        "    for _ in tqdm.tqdm(range(100)):\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      x, y = get_batch(\n",
        "          random_subkey, eval_data, batch_size=batch_size, block_size=block_size)\n",
        "      batch_eval_loss = eval_step(mini_gpt_state, x, y)\n",
        "      eval_losses.append(batch_eval_loss)\n",
        "    print(f\"Average eval loss: {np.mean(eval_losses)}\")\n",
        "    self.assertTrue(np.mean(eval_losses) < 1.9)\n",
        "\n",
        "# Uncomment to execute test.\n",
        "# TestTask1Solution().test_minigpt()"
      ],
      "metadata": {
        "id": "Z1hVeNo9qN_B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution for task 2.\n",
        "\n",
        "class AttentionTask2Solution(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x) # B, T, head_dim\n",
        "    k = self.key(x) # B, T, head_dim\n",
        "    wei = q @ jax.numpy.transpose(k, axes=(0, 2, 1)) # B, T, T\n",
        "    mask = jnp.tril(jnp.ones((T, T)))\n",
        "    wei = jnp.where(mask, wei, -jnp.inf)\n",
        "    wei = nn.softmax(wei / jnp.sqrt(self.head_dim), axis=-1) # B, T, T\n",
        "    return wei @ self.value(x) # B, T, C\n",
        "\n",
        "# This is a duplicate of TestAttention. The only difference is that the test\n",
        "# runs AttentionTask2Solution instead of Attention.\n",
        "class TestAttention(unittest.TestCase):\n",
        "\n",
        "  EXPECTED_ATTENTION_ARRAY = np.array([\n",
        "    [[-0.3368626, 0.1565489, 0.96250117, 0.7116083, 0.48668504,\n",
        "      0.3070267, -0.49149823, 0.7827484, 0.4131582, 0.7505922,\n",
        "      0.90185213, -0.34802976, 1.2631372, 0.8314824, 0.45534268,\n",
        "      0.11072167],\n",
        "     [0.355573, 0.36409345, 0.19864899, 0.58222437, -0.01833684,\n",
        "      0.8821246, 0.26334122, 0.10999514, 0.69409794, 0.3437622,\n",
        "      -0.71399987, 0.6530971, 0.00235165, -0.5397035, 0.55874693,\n",
        "      -0.4885986],\n",
        "     [0.6003635, 0.34785143, -0.25671193, 0.3002994, -0.31720588,\n",
        "      1.2125036, 0.6570689, -0.22460055, 0.9200514, -0.01703957,\n",
        "      -1.5395278, 1.1767541, -0.7460983, -1.3350787, 0.61231965,\n",
        "      -1.0458561],\n",
        "     [-0.7845163, -0.5571454, 0.39112994, -0.63247937, -0.2971205,\n",
        "      0.19273886, -0.25068092, 0.5804176, 0.3952121, 0.24023446,\n",
        "      1.1744585, -1.0228857, 1.0987606, 0.90741533, 0.19215004,\n",
        "      -0.98253024]]\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  def test_attention(self):\n",
        "    attention = AttentionTask2Solution(head_dim=16)\n",
        "    params = attention.init(jax.random.key(0), jnp.ones((1, 4, 8)))\n",
        "    x = jax.random.normal(key=jax.random.key(0), shape=(1, 4, 8), dtype=jnp.float32)\n",
        "    y = attention.apply(params, x)\n",
        "    self.assertTrue(np.allclose(y, self.EXPECTED_ATTENTION_ARRAY))\n",
        "\n",
        "TestAttention().test_attention()"
      ],
      "metadata": {
        "id": "pQEWUSZ2pPF7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is only used for the next task.\n",
        "class MultiHeadAttentionTask2Solution(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [AttentionTask2Solution(self.head_dim) for _ in range(self.num_heads)]\n",
        "    self.dense = nn.Dense(self.num_heads*self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n",
        "    return self.dense(x)  # B, T, hidden_dim\n"
      ],
      "metadata": {
        "id": "A3pGmrUZy4q5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution for task 3\n",
        "class MultiHeadAttentionTask3Solution(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    # Don't setup function.\n",
        "    self.query = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.dense = nn.Dense(features=self.num_heads * self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    # Compute queries, keys, and values\n",
        "    q = self.query(x)  # (B, T, num_heads*head_dim)\n",
        "    k = self.key(x)  # (B, T, num_heads*head_dim)\n",
        "    v = self.value(x)  # (B, T, num_heads*head_dim)\n",
        "\n",
        "    # Reshape to (B, T, num_heads, head_dim) and transpose to (B, num_heads, T, head_dim)\n",
        "    q = q.reshape(B, T, self.num_heads, self.head_dim).transpose((0, 2, 1, 3))\n",
        "    k = k.reshape(B, T, self.num_heads, self.head_dim).transpose((0, 2, 1, 3))\n",
        "    v = v.reshape(B, T, self.num_heads, self.head_dim).transpose((0, 2, 1, 3))\n",
        "\n",
        "    # Compute scaled dot-product attention using einsum\n",
        "    wei = jnp.einsum('bnth,bnsh->bnts', q, k) / jnp.sqrt(self.head_dim)  # (B, num_heads, T, T)\n",
        "\n",
        "    mask = jnp.tril(jnp.ones((T, T)))\n",
        "    wei = jnp.where(mask, wei, -jnp.inf)\n",
        "    wei = nn.softmax(wei, axis=-1)  # (B, num_heads, T, T)\n",
        "\n",
        "    # Compute the output using einsum\n",
        "    out = jnp.einsum('bnts,bnsh->bnth', wei, v)  # (B, num_heads, T, head_dim)\n",
        "\n",
        "    # Reshape to (B, T, num_heads*head_dim) for the final dense layer\n",
        "    out = out.transpose((0, 2, 1, 3)).reshape(B, T, self.num_heads * self.head_dim)\n",
        "\n",
        "    return self.dense(out)  # B, T, C\n",
        "\n",
        "def measure_attention_time(\n",
        "    batch_size, seq_length, num_heads, head_dim, model, num_iterations=20):\n",
        "  total_time = 0.0\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "  x = jax.random.normal(rng, (batch_size, seq_length, num_heads * head_dim))\n",
        "  params = model.init(rng, x)\n",
        "  jitted_apply = jax.jit(model.apply)\n",
        "  # The first run is for compiling Jax. We'll ignore it.\n",
        "  _ = jitted_apply(params, x)\n",
        "\n",
        "  for _ in tqdm.tqdm(range(num_iterations)):\n",
        "    start_time = time.time()\n",
        "    jitted_apply(params, x)\n",
        "    end_time = time.time()\n",
        "    total_time += end_time - start_time\n",
        "  average_attention_time = total_time / num_iterations\n",
        "  print(f\"Average inference time: {average_attention_time} seconds\")\n",
        "  return average_attention_time\n"
      ],
      "metadata": {
        "id": "eTJNqKlhB3-W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestMultiHeadEinsum(unittest.TestCase):\n",
        "  def test_multihead_einsum(self):\n",
        "    head_dim = 4\n",
        "    num_heads = 2\n",
        "    batch_size = 1\n",
        "    seq_length = 2\n",
        "    hidden_dim = head_dim * num_heads\n",
        "    random_key = jax.random.PRNGKey(0)\n",
        "\n",
        "    new_model = MultiHeadAttentionTask3Solution(\n",
        "        head_dim=head_dim,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "\n",
        "    new_params = new_model.init(random_key, jnp.ones((batch_size, seq_length, hidden_dim), dtype=jnp.int16))\n",
        "\n",
        "    baseline_model = MultiHeadAttentionTask2Solution(\n",
        "        head_dim=head_dim,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "    baseline_params = baseline_model.init(random_key, jnp.ones((batch_size, seq_length, hidden_dim), dtype=jnp.int16))\n",
        "\n",
        "    baseline_params['params']['heads_0']['query']['kernel'] = new_params['params']['query']['kernel'][:, :4].copy()\n",
        "    baseline_params['params']['heads_0']['key']['kernel'] = new_params['params']['key']['kernel'][:, :4].copy()\n",
        "    baseline_params['params']['heads_0']['value']['kernel'] = new_params['params']['value']['kernel'][:, :4].copy()\n",
        "    baseline_params['params']['heads_1']['query']['kernel'] = new_params['params']['query']['kernel'][:, 4:].copy()\n",
        "    baseline_params['params']['heads_1']['key']['kernel'] = new_params['params']['key']['kernel'][:, 4:].copy()\n",
        "    baseline_params['params']['heads_1']['value']['kernel'] = new_params['params']['value']['kernel'][:, 4:].copy()\n",
        "    baseline_params['params']['dense']['kernel'] = new_params['params']['dense']['kernel'].copy()\n",
        "    baseline_params['params']['dense']['bias'] = new_params['params']['dense']['bias'].copy()\n",
        "\n",
        "    baseline_res = baseline_model.apply(baseline_params, jnp.ones((batch_size, seq_length, hidden_dim), dtype=jnp.int16))\n",
        "    new_res = new_model.apply(new_params, jnp.ones((batch_size, seq_length, hidden_dim), dtype=jnp.int16))\n",
        "    self.assertTrue(np.allclose(new_res, baseline_res))\n",
        "\n",
        "TestMultiHeadEinsum().test_multihead_einsum()"
      ],
      "metadata": {
        "id": "LuyNnmY-ybXc"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSmAJzKZyIf1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}