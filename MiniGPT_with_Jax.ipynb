{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tested with free Google Compute Engine Backend. No GPU required."
      ],
      "metadata": {
        "id": "_QeF9jIUwxoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ZePpgW6jLvja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5olYdwfKYI-"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pp\n",
        "import tqdm\n",
        "import unittest\n",
        "import time\n",
        "import functools\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "AhxprQ-fKq9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions\n",
        "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
        "\n",
        "def get_batch(random_key, data, batch_size, block_size):\n",
        "  \"\"\"Generate a batch of data of inputs x and targets y.\n",
        "\n",
        "  Args:\n",
        "    random_key (jax.random.PRNGKey): Random number generator key.\n",
        "    data (array-like): 1d JAX array of integer tokens\n",
        "    batch_size (int): Batch size.\n",
        "    block_size (int): The maximum input context length.\n",
        "\n",
        "  Returns:\n",
        "    x (array-like): 2d JAX array of shape (batch_size, block_size).\n",
        "    y (array-like): 2d JAX array of shape (batch_size, block_size).\n",
        "        x[i, j] == y[i, j-1] where j > 0.\n",
        "  \"\"\"\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  ix = jax.random.randint(random_key, shape=(batch_size, 1), minval=0, maxval=len(data)-block_size)\n",
        "  x = dynamic_slice_vmap(data, ix, (block_size,))\n",
        "  y = dynamic_slice_vmap(data, ix+1, (block_size,))\n",
        "  return x, y\n",
        "\n",
        "def load_shakespeare_dataset():\n",
        "  with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "  data = jnp.array(encode(text))\n",
        "  n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "  train_data = data[:n]\n",
        "  eval_data = data[n:]\n",
        "  return train_data, eval_data\n",
        "\n",
        "def init_train_state(\n",
        "    model,\n",
        "    params,\n",
        "    learning_rate=1e-4,\n",
        "):\n",
        "  tx = optax.adam(learning_rate)\n",
        "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, x, y):\n",
        "  \"\"\"Run one step of training.\n",
        "  Args:\n",
        "    state (jax.training.TrainState): Jax TrainState containing weights and\n",
        "      optimizer states.\n",
        "    x (array-like): 2d JAX int array of shape (batch_size, block_size).\n",
        "    y (array-like): 2d JAX int array of shape (batch_size, block_size).\n",
        "\n",
        "  Returns:\n",
        "    state (jax.training.TrainState): The new train state after applying\n",
        "      gradient descent on weights and updating optimizer states.\n",
        "    loss (float): Loss for this training step.\n",
        "  \"\"\"\n",
        "  def _loss(params):\n",
        "    predictions = state.apply_fn(params, x) # B, T, vocab_size\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(predictions, y)\n",
        "    return loss.mean()\n",
        "  loss, grads = jax.value_and_grad(_loss)(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, loss\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(state, x, y):\n",
        "  predictions = state.apply_fn(state.params, x)\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(predictions, y).mean()\n",
        "\n",
        "def run_training_loop(\n",
        "    num_iterations,\n",
        "    batch_size,\n",
        "    block_size,\n",
        "    learning_rate,\n",
        "    eval_data,\n",
        "    train_data,\n",
        "    model,\n",
        "):\n",
        "  \"\"\"\n",
        "  Runs the training loop for the specified model.\n",
        "\n",
        "  Args:\n",
        "      num_iterations (int): The number of training iterations.\n",
        "      batch_size (int): The number of samples in each batch.\n",
        "      block_size (int): The size of each block (sequence length).\n",
        "      learning_rate (float): The learning rate for the optimizer.\n",
        "      eval_data (array-like): 1d JAX array of integer tokens, consisting of evaluation data.\n",
        "      train_data (array-like): 1d JAX array of integer tokens, consisting of training data.\n",
        "      model (nn.Module, optional): A Jax Model object.\n",
        "\n",
        "  Returns:\n",
        "      state: The training state with the best eval metrics.\n",
        "\n",
        "  Example:\n",
        "      >>> final_state = run_training_loop(\n",
        "      >>>     num_iterations=1000,\n",
        "      >>>     batch_size=16,\n",
        "      >>>     block_size=32,\n",
        "      >>>     learning_rate=0.001,\n",
        "      >>>     eval_data=eval_data,\n",
        "      >>>     train_data=train_data,\n",
        "      >>>     model=mini_gpt\n",
        "      >>> )\n",
        "  \"\"\"\n",
        "  random_key = jax.random.PRNGKey(0)\n",
        "  x = jnp.ones((batch_size, block_size), dtype=jnp.int16)\n",
        "  random_key, random_subkey = jax.random.split(random_key)\n",
        "  params = model.init(random_subkey, x)\n",
        "  state = init_train_state(\n",
        "      model, params, learning_rate=learning_rate)\n",
        "  predictions = state.apply_fn(state.params, x)\n",
        "  best_state = state\n",
        "  best_eval_loss = math.inf\n",
        "  for i in range(num_iterations):\n",
        "    random_key, random_subkey = jax.random.split(random_key)\n",
        "    x, y = get_batch(random_subkey, train_data, batch_size=batch_size, block_size=block_size)\n",
        "    state, loss = train_step(state, x, y)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      eval_loss = eval_step(state, *get_batch(random_subkey, eval_data, batch_size=batch_size, block_size=block_size))\n",
        "      print(f\"Step: {i}\\t train loss: {loss}\\t eval loss: {eval_loss}\")\n",
        "      if eval_loss < best_eval_loss:\n",
        "        best_eval_loss = eval_loss\n",
        "        best_state = state\n",
        "  return best_state"
      ],
      "metadata": {
        "id": "jIoEU_PAkk9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and tokenize dataset"
      ],
      "metadata": {
        "id": "7bhOfS-TL0iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "id": "Wqg4qFTxK3W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "data = jnp.array(encode(text))\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "eval_data = data[n:]"
      ],
      "metadata": {
        "id": "m-fbEt4DLNGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warm up - Check the performance of a simple text decoder model\n",
        "\n",
        "The SimpleDecoder below will predict the next token given a single token."
      ],
      "metadata": {
        "id": "qu4if-0aLfAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDecoder(nn.Module):\n",
        "  vocab_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.vocab_size)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    return self.token_embedding(x) # B, T, vocab_size\n",
        "\n",
        "  def generate(self, start_token, max_length=20, end_token=None):\n",
        "    # Initialize the generated sequence with the start token\n",
        "    generated_sequence = [start_token]\n",
        "    current_token = start_token\n",
        "\n",
        "    for _ in range(max_length - 1):  # We already have the start token\n",
        "      # Convert the current token to a tensor\n",
        "      current_token_tensor = jnp.array([[current_token]])\n",
        "\n",
        "      # Get the token embeddings\n",
        "      token_logits = self.__call__(current_token_tensor)\n",
        "\n",
        "      # Get the token with the highest probability\n",
        "      next_token = jnp.argmax(token_logits, axis=-1)[0]\n",
        "\n",
        "      # Append the next token to the generated sequence\n",
        "      generated_sequence.append(int(next_token[0]))\n",
        "\n",
        "      # If the end token is generated, stop the generation\n",
        "      if end_token is not None and next_token[0] == end_token:\n",
        "          break\n",
        "\n",
        "      # Update the current token\n",
        "      current_token = int(next_token[0])\n",
        "\n",
        "    return generated_sequence\n",
        "\n",
        "decoder = SimpleDecoder(vocab_size=vocab_size)\n",
        "start_token = 23\n",
        "dummy = jnp.ones((4, 8), dtype=jnp.int16)\n",
        "params = decoder.init(jax.random.PRNGKey(0), dummy)\n",
        "\n",
        "# Generate a sequence\n",
        "generated_sequence = decoder.apply(params, start_token, method=decoder.generate, max_length=20)\n",
        "print(\"Generated sequence:\", decode(generated_sequence))\n"
      ],
      "metadata": {
        "id": "KYMAEy19OUsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Generated sequence is gibberish. Let's see if it gets better when we train it."
      ],
      "metadata": {
        "id": "sl79uLx4QXd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can play around the parameters here to see how that affects loss.\n",
        "num_iterations = 7000\n",
        "learning_rate = 1e-3\n",
        "num_layers = 4\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "num_heads = 4\n",
        "hidden_dim = 64\n",
        "\n",
        "decoder = SimpleDecoder(vocab_size=vocab_size)\n",
        "\n",
        "simple_decoder_state = run_training_loop(\n",
        "    num_iterations = num_iterations,\n",
        "    learning_rate = learning_rate,\n",
        "    batch_size = batch_size,\n",
        "    block_size = block_size,\n",
        "    eval_data = eval_data,\n",
        "    train_data = train_data,\n",
        "    model = decoder\n",
        ")"
      ],
      "metadata": {
        "id": "zyGvbsMHQrwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sequence = decoder.apply(simple_decoder_state.params, start_token, method=decoder.generate, max_length=20)\n",
        "print(\"Generated sequence:\", decode(generated_sequence))\n"
      ],
      "metadata": {
        "id": "xE_VcrDJHBmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Implement MiniGPT.\n",
        "\n",
        "* You can use off-the-shelf Flax modules like Dense, LayerNorm. You may not use Flax's SelfAttention. Instead, use AttentionTask1 provided below.\n",
        "* Note that block_size, T, input context window length are different ways to refer to the same thing."
      ],
      "metadata": {
        "id": "5jhrHj0xLB_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B == batch_size.\n",
        "# T == number of tokens in sequence.\n",
        "# C == hidden_dim == hidden dimension of transformer.\n",
        "# head_dim == Head dimension for each Attention head. head_dim * num_heads == C.\n",
        "\n",
        "# You can use this class for solving Task 1. We will revisit this class in Task 2.\n",
        "class AttentionTask1(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.attention_impl = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=1, qkv_features=self.head_dim, dropout_rate=0.)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # x is of shape B, T, C.\n",
        "    q = self.query(x)  # B, T, head_dim\n",
        "    k = self.key(x)  # B, T, head_dim\n",
        "    v = self.value(x)  # B, T, head_dim\n",
        "    return self.attention_impl(inputs_q=q, inputs_k=k, inputs_v=v)  # B, T, head_dim\n",
        "\n",
        "# FeedForward is given to you for free.\n",
        "class FeedForward(nn.Module):\n",
        "  hidden_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.f1 = nn.Dense(features=4 * self.hidden_dim)\n",
        "    self.f2 = nn.Dense(features=self.hidden_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.f2(nn.relu(self.f1(x)))  # B, T, hidden_dim\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [AttentionTask1(self.head_dim) for _ in range(self.num_heads)]\n",
        "    self.dense = nn.Dense(self.num_heads*self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # TODO: Implement multi-head attention.\n",
        "    return self.dense(x)  # B, T, hidden_dim\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  hidden_dim: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    # head_dim * num_heads == hidden_dim should always hold true.\n",
        "    head_dim = self.hidden_dim // self.num_heads\n",
        "    # TODO: Fill out the rest of setup function.\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # TODO: Implement this function.\n",
        "    return x  # B, T, hidden_dim\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "  vocab_size: int\n",
        "  hidden_dim: int\n",
        "  block_size: int\n",
        "  num_layers: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.hidden_dim)\n",
        "    self.position_encoding = nn.Embed(\n",
        "        num_embeddings=self.block_size,\n",
        "        features=self.hidden_dim\n",
        "    )\n",
        "    self.final_dense = nn.Dense(features=self.vocab_size)\n",
        "    # TODO: Fill out the rest of this function.\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.token_embedding(x)  # B, T, hidden_dim\n",
        "\n",
        "    # TODO: Fill in missing functionalities here.\n",
        "\n",
        "    return self.final_dense(x)\n",
        "\n",
        "  def generate(self, random_key, params, x, max_new_tokens=50):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits = self.apply(params, x[:, -self.block_size:])\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      new_token = jax.random.categorical(random_subkey, logits[:, -1, :], axis=-1, shape=None)\n",
        "      x = jnp.concatenate([x, new_token[:, None]], axis=1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lsjrh9ZhOgzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can play around the parameters here to see how that affects loss.\n",
        "num_iterations = 4000\n",
        "learning_rate = 1e-3\n",
        "num_layers = 4\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "num_heads = 4\n",
        "hidden_dim = 128\n",
        "\n",
        "mini_gpt = MiniGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_dim=hidden_dim,\n",
        "    block_size=block_size,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads\n",
        ")\n",
        "\n",
        "mini_gpt_state = run_training_loop(\n",
        "    num_iterations=num_iterations,\n",
        "    learning_rate=learning_rate,\n",
        "    batch_size=batch_size,\n",
        "    block_size=block_size,\n",
        "    eval_data=eval_data,\n",
        "    train_data=train_data,\n",
        "    model=mini_gpt\n",
        ")"
      ],
      "metadata": {
        "id": "emkzC15clmUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment below to print predictions:\n",
        "# x = jnp.zeros((1, 1), dtype=jnp.int32)\n",
        "# random_key = jax.random.PRNGKey(0)\n",
        "# tokens = mini_gpt.generate(random_key, params=mini_gpt_state.params, x=x)\n",
        "# print(decode(tokens[0].tolist()))"
      ],
      "metadata": {
        "id": "gyB7K_wM6rXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass this test before moving on to Task 2.\n",
        "class TestTask1(unittest.TestCase):\n",
        "\n",
        "  def test_minigpt(self):\n",
        "    # Do not change these parameters.\n",
        "    num_iterations = 4000\n",
        "    learning_rate = 1e-3\n",
        "    num_layers = 4\n",
        "    batch_size = 16\n",
        "    block_size = 32\n",
        "    num_heads = 4\n",
        "    hidden_dim = 128\n",
        "    random_key = jax.random.PRNGKey(42)\n",
        "\n",
        "    mini_gpt = MiniGPT(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_dim=hidden_dim,\n",
        "        block_size=block_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "\n",
        "    train_data, eval_data = load_shakespeare_dataset()\n",
        "    mini_gpt_state = run_training_loop(\n",
        "        num_iterations = num_iterations,\n",
        "        learning_rate = learning_rate,\n",
        "        batch_size = batch_size,\n",
        "        block_size = block_size,\n",
        "        eval_data = eval_data,\n",
        "        train_data = train_data,\n",
        "        model = mini_gpt\n",
        "    )\n",
        "    eval_losses = []\n",
        "    for _ in tqdm.tqdm(range(100)):\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      x, y = get_batch(\n",
        "          random_subkey, eval_data, batch_size=batch_size, block_size=block_size)\n",
        "      batch_eval_loss = eval_step(mini_gpt_state, x, y)\n",
        "      eval_losses.append(batch_eval_loss)\n",
        "    print(f\"Average eval loss: {np.mean(eval_losses)}\")\n",
        "    self.assertTrue(np.mean(eval_losses) < 1.9)\n",
        "\n",
        "# Uncomment the test below.\n",
        "# TestTask1().test_minigpt()"
      ],
      "metadata": {
        "id": "cleRdSmmvDSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - implement the Self-Attention Jax Module\n",
        "\n",
        "Your task is to implement Attention without using Flax's built-in nn.MultiHeadDotProductAttention module. Fill in the TODO section below.\n",
        "\n",
        "Things to keep in mind:\n",
        "\n",
        "* We are implementing a decoder-only transformer. This means that each token can only attend to previous tokens, but not future tokens."
      ],
      "metadata": {
        "id": "aezQPKA3OXLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionTask2(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    # Don't change the setup function.\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    # TODO: This function contains an incorrect attention implmentation. Change\n",
        "    # its definition below:\n",
        "    B, T, C = x.shape\n",
        "    return self.query(x) + self.key(x) + self.value(x) # B, T, head_dim"
      ],
      "metadata": {
        "id": "r0aff872OvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestAttention(unittest.TestCase):\n",
        "\n",
        "  EXPECTED_ATTENTION_ARRAY = np.array([[\n",
        "      [-2.3660736,  -1.0994253,   0.54647386,  1.663486,    1.0262686,\n",
        "      0.50164324, -0.40740347, -0.86529493,  1.6112939,  -0.46789974,\n",
        "      1.3150474,   0.9799258,  -0.5418715,  -1.2731858,  -0.7926506,\n",
        "      -0.8737542],\n",
        "      [-1.2626604,   1.3287369,   0.96550566,  0.4553011,   0.6900299,\n",
        "      -0.6283262,  -0.44400188,  0.18089633, -0.6977915,  -0.49270085,\n",
        "      0.1377207,   0.19912332, -0.02095406, -1.0335875,  -0.13449836,\n",
        "      -0.9766264],\n",
        "      [-2.1633344,  -0.7197231,   0.59619266,  1.4519494,   0.9575919,\n",
        "      0.33423916, -0.39966965, -0.69272554,  1.2452503,  -0.46386948,\n",
        "      1.1163911,   0.84217906, -0.45448378, -1.2067673,  -0.6738551,\n",
        "      -0.87238634],\n",
        "      [ 0.21197765,  0.2127537,  -0.27920845, -0.4683921,  -0.22381224,\n",
        "      0.49012795,  0.44253582,  0.2606917,   0.03008281,  0.06132472,\n",
        "      -0.28707987, -0.4550119,   0.16932811,  0.7396863,   0.54958737,\n",
        "      0.23469326]\n",
        "  ]])\n",
        "\n",
        "  def test_attention(self):\n",
        "    attention = AttentionTask2(head_dim=16)\n",
        "    key = jax.random.PRNGKey(0)\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params = attention.init(subkey, jnp.ones((1, 4, 8)))\n",
        "    x = jax.random.normal(key=key, shape=(1, 4, 8), dtype=jnp.float32)\n",
        "    y = attention.apply(params, x)\n",
        "    self.assertTrue(np.allclose(y, self.EXPECTED_ATTENTION_ARRAY))\n",
        "\n",
        "# Uncomment the test below.\n",
        "TestAttention().test_attention()"
      ],
      "metadata": {
        "id": "C3WxOMkcaV4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 Speed up MultiheadAttention with Einsum.\n",
        "\n",
        "Please finish task 2 first before doing this task."
      ],
      "metadata": {
        "id": "uAWkbRLV7gp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionTask3(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.dense = nn.Dense(features=self.num_heads * self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    # TODO: Implement this using Einsum.\n",
        "\n",
        "    return self.query(x) + self.key(x) + self.value(x) # B, T, head_dim\n"
      ],
      "metadata": {
        "id": "oAktMykm5TMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestMultiHeadEinsum(unittest.TestCase):\n",
        "  EXPECTED_ATTENTION_ARRAY = np.array([[\n",
        "      [-2.3660736,  -1.0994253,   0.54647386,  1.663486,    1.0262686,\n",
        "      0.50164324, -0.40740347, -0.86529493,  1.6112939,  -0.46789974,\n",
        "      1.3150474,   0.9799258,  -0.5418715,  -1.2731858,  -0.7926506,\n",
        "      -0.8737542],\n",
        "      [-1.2626604,   1.3287369,   0.96550566,  0.4553011,   0.6900299,\n",
        "      -0.6283262,  -0.44400188,  0.18089633, -0.6977915,  -0.49270085,\n",
        "      0.1377207,   0.19912332, -0.02095406, -1.0335875,  -0.13449836,\n",
        "      -0.9766264],\n",
        "      [-2.1633344,  -0.7197231,   0.59619266,  1.4519494,   0.9575919,\n",
        "      0.33423916, -0.39966965, -0.69272554,  1.2452503,  -0.46386948,\n",
        "      1.1163911,   0.84217906, -0.45448378, -1.2067673,  -0.6738551,\n",
        "      -0.87238634],\n",
        "      [ 0.21197765,  0.2127537,  -0.27920845, -0.4683921,  -0.22381224,\n",
        "      0.49012795,  0.44253582,  0.2606917,   0.03008281,  0.06132472,\n",
        "      -0.28707987, -0.4550119,   0.16932811,  0.7396863,   0.54958737,\n",
        "      0.23469326]\n",
        "  ]])\n",
        "\n",
        "  def test_multihead_einsum(self):\n",
        "    attention_einsum = MultiHeadAttentionTask3(num_heads=2, head_dim=8)\n",
        "    key = jax.random.PRNGKey(0)\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params = attention_einsum.init(subkey, jnp.ones((1, 4, 16)))\n",
        "    x = jax.random.normal(key=key, shape=(1, 4, 16), dtype=jnp.float32)\n",
        "    y = attention_einsum.apply(params, x)\n",
        "    self.assertTrue(np.allclose(y, self.EXPECTED_ATTENTION_ARRAY))\n",
        "\n",
        "# TestMultiHeadEinsum().test_multihead_einsum()"
      ],
      "metadata": {
        "id": "bApnTKK-MQhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Rerun the training loop using MultiHeadAttentionTask3. Can you still achieve the same eval results?"
      ],
      "metadata": {
        "id": "a2s7pU3F0E5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "* Run the Import section at the beginning of this colab before running the Solution.\n",
        "* The Solutions below need to be executed sequentially."
      ],
      "metadata": {
        "id": "i1RfdW6UpM9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution for task 1.\n",
        "class ReferenceAttention(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.attention_impl = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=1, qkv_features=self.head_dim, dropout_rate=0.)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x)  # B, T, head_dim\n",
        "    k = self.key(x)  # B, T, head_dim\n",
        "    v = self.value(x)  # B, T, head_dim\n",
        "    mask = jnp.tril(jnp.ones((B, 1, T, T)))\n",
        "    return self.attention_impl(inputs_q=q, inputs_k=k, inputs_v=v, mask=mask)  # B, T, head_dim\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  hidden_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.f1 = nn.Dense(features=4 * self.hidden_dim)\n",
        "    self.f2 = nn.Dense(features=self.hidden_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.f2(nn.relu(self.f1(x)))  # B, T, hidden_dim\n",
        "\n",
        "class MultiHeadAttentionSolution(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [ReferenceAttention(self.head_dim) for _ in range(self.num_heads)]\n",
        "    self.dense = nn.Dense(self.num_heads*self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n",
        "    return self.dense(x)  # B, T, hidden_dim\n",
        "\n",
        "class DecoderBlockSolution(nn.Module):\n",
        "  hidden_dim: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    head_dim = self.hidden_dim // self.num_heads\n",
        "    self.mha = MultiHeadAttentionSolution(\n",
        "        num_heads=self.num_heads,\n",
        "        head_dim=head_dim)\n",
        "    self.ff = FeedForward(self.hidden_dim)\n",
        "    self.ff_norm = nn.LayerNorm()\n",
        "    self.mha_norm = nn.LayerNorm()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = x + self.mha(self.mha_norm(x))\n",
        "    return x + self.ff(self.ff_norm(x))\n",
        "\n",
        "\n",
        "class MiniGPTSolution(nn.Module):\n",
        "  vocab_size: int\n",
        "  hidden_dim: int\n",
        "  block_size: int\n",
        "  num_layers: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.hidden_dim)\n",
        "    self.position_encoding = nn.Embed(\n",
        "        num_embeddings=self.block_size,\n",
        "        features=self.hidden_dim\n",
        "    )\n",
        "    self.decoder_blocks = [\n",
        "        DecoderBlockSolution(self.hidden_dim, self.num_heads) for _ in range(self.num_layers)\n",
        "    ]\n",
        "    self.final_norm = nn.LayerNorm()\n",
        "    self.final_dense = nn.Dense(features=self.vocab_size)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.token_embedding(x)  # B, T, hidden_dim\n",
        "    pos = self.position_encoding(jnp.arange(T))  # T, hidden_dim\n",
        "    x += pos\n",
        "    for block in self.decoder_blocks:\n",
        "      x = block(x)\n",
        "    return self.final_dense(self.final_norm(x))\n",
        "\n",
        "  def generate(self, random_key, params, x, max_new_tokens=50):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits = self.apply(params, x[:, -self.block_size:])\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      new_token = jax.random.categorical(random_subkey, logits[:, -1, :], axis=-1, shape=None)\n",
        "      x = jnp.concatenate([x, new_token[:, None]], axis=1)\n",
        "    return x\n",
        "\n",
        "# This is a duplicate of TestTask1.\n",
        "class TestTask1Solution(unittest.TestCase):\n",
        "\n",
        "  def test_minigpt(self):\n",
        "    num_iterations = 4000\n",
        "    learning_rate = 1e-3\n",
        "    num_layers = 4\n",
        "    batch_size = 16\n",
        "    block_size = 32\n",
        "    num_heads = 4\n",
        "    hidden_dim = 128\n",
        "    random_key = jax.random.PRNGKey(42)\n",
        "\n",
        "    model = MiniGPTSolution(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_dim=hidden_dim,\n",
        "        block_size=block_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "    train_data, eval_data = load_shakespeare_dataset()\n",
        "    mini_gpt_state = run_training_loop(\n",
        "        num_iterations = num_iterations,\n",
        "        learning_rate = learning_rate,\n",
        "        batch_size = batch_size,\n",
        "        block_size = block_size,\n",
        "        eval_data = eval_data,\n",
        "        train_data = train_data,\n",
        "        model = model\n",
        "    )\n",
        "    eval_losses = []\n",
        "    for _ in tqdm.tqdm(range(100)):\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      x, y = get_batch(\n",
        "          random_subkey, eval_data, batch_size=batch_size, block_size=block_size)\n",
        "      batch_eval_loss = eval_step(mini_gpt_state, x, y)\n",
        "      eval_losses.append(batch_eval_loss)\n",
        "    print(f\"Average eval loss: {np.mean(eval_losses)}\")\n",
        "    self.assertTrue(np.mean(eval_losses) < 1.9)\n",
        "\n",
        "# Uncomment to execute test.\n",
        "TestTask1Solution().test_minigpt()"
      ],
      "metadata": {
        "id": "Z1hVeNo9qN_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution for task 2.\n",
        "\n",
        "class AttentionTask2Solution(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x) # B, T, head_dim\n",
        "    k = self.key(x) # B, T, head_dim\n",
        "    wei = q @ jax.numpy.transpose(k, axes=(0, 2, 1)) # B, T, T\n",
        "    mask = jnp.tril(jnp.ones((T, T)))\n",
        "    wei = jnp.where(mask, wei, -jnp.inf)\n",
        "    wei = nn.softmax(wei / jnp.sqrt(self.head_dim), axis=-1) # B, T, T\n",
        "    return wei @ self.value(x) # B, T, C\n",
        "\n",
        "# This is a duplicate of TestAttention. The only difference is that the test\n",
        "# runs AttentionTask2Solution instead of Attention.\n",
        "class TestAttention(unittest.TestCase):\n",
        "\n",
        "  EXPECTED_ATTENTION_ARRAY = np.array([\n",
        "    [[-0.3368626, 0.1565489, 0.96250117, 0.7116083, 0.48668504,\n",
        "      0.3070267, -0.49149823, 0.7827484, 0.4131582, 0.7505922,\n",
        "      0.90185213, -0.34802976, 1.2631372, 0.8314824, 0.45534268,\n",
        "      0.11072167],\n",
        "     [0.355573, 0.36409345, 0.19864899, 0.58222437, -0.01833684,\n",
        "      0.8821246, 0.26334122, 0.10999514, 0.69409794, 0.3437622,\n",
        "      -0.71399987, 0.6530971, 0.00235165, -0.5397035, 0.55874693,\n",
        "      -0.4885986],\n",
        "     [0.6003635, 0.34785143, -0.25671193, 0.3002994, -0.31720588,\n",
        "      1.2125036, 0.6570689, -0.22460055, 0.9200514, -0.01703957,\n",
        "      -1.5395278, 1.1767541, -0.7460983, -1.3350787, 0.61231965,\n",
        "      -1.0458561],\n",
        "     [-0.7845163, -0.5571454, 0.39112994, -0.63247937, -0.2971205,\n",
        "      0.19273886, -0.25068092, 0.5804176, 0.3952121, 0.24023446,\n",
        "      1.1744585, -1.0228857, 1.0987606, 0.90741533, 0.19215004,\n",
        "      -0.98253024]]\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  def test_attention(self):\n",
        "    attention = AttentionTask2Solution(head_dim=16)\n",
        "    params = attention.init(jax.random.key(0), jnp.ones((1, 4, 8)))\n",
        "    x = jax.random.normal(key=jax.random.key(0), shape=(1, 4, 8), dtype=jnp.float32)\n",
        "    y = attention.apply(params, x)\n",
        "    self.assertTrue(np.allclose(y, self.EXPECTED_ATTENTION_ARRAY))\n",
        "\n",
        "TestAttention().test_attention()"
      ],
      "metadata": {
        "id": "pQEWUSZ2pPF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution for task 3\n",
        "# This is only used for the next task.\n",
        "class MultiHeadAttentionReferenceTask3(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [AttentionTask2Solution(self.head_dim) for _ in range(self.num_heads)]\n",
        "    self.dense = nn.Dense(self.num_heads*self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n",
        "    return self.dense(x)  # B, T, hidden_dim\n",
        "\n",
        "class MultiHeadAttentionTask3Solution(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.dense = nn.Dense(features=self.num_heads * self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x)  # (B, T, num_heads*head_dim)\n",
        "    k = self.key(x)  # (B, T, num_heads*head_dim)\n",
        "    v = self.value(x)  # (B, T, num_heads*head_dim)\n",
        "    q = q.reshape(B, T, self.num_heads, self.head_dim).transpose((0, 2, 1, 3))\n",
        "    k = k.reshape(B, T, self.num_heads, self.head_dim).transpose((0, 2, 1, 3))\n",
        "    v = v.reshape(B, T, self.num_heads, self.head_dim).transpose((0, 2, 1, 3))\n",
        "    wei = jnp.einsum('bnth,bnsh->bnts', q, k) / jnp.sqrt(self.head_dim)  # (B, num_heads, T, T)\n",
        "    mask = jnp.tril(jnp.ones((T, T)))\n",
        "    wei = jnp.where(mask, wei, -jnp.inf)\n",
        "    wei = nn.softmax(wei, axis=-1)  # (B, num_heads, T, T)\n",
        "    out = jnp.einsum('bnts,bnsh->bnth', wei, v)  # (B, num_heads, T, head_dim)\n",
        "    out = out.transpose((0, 2, 1, 3)).reshape(B, T, self.num_heads * self.head_dim)\n",
        "    return self.dense(out)  # B, T, C\n",
        "\n",
        "def measure_attention_time(\n",
        "    batch_size, seq_length, num_heads, head_dim, model, num_iterations=20):\n",
        "  total_time = 0.0\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "  x = jax.random.normal(rng, (batch_size, seq_length, num_heads * head_dim))\n",
        "  params = model.init(rng, x)\n",
        "  jitted_apply = jax.jit(model.apply)\n",
        "  # The first run is for compiling Jax. We'll ignore it.\n",
        "  _ = jitted_apply(params, x)\n",
        "\n",
        "  for _ in tqdm.tqdm(range(num_iterations)):\n",
        "    start_time = time.time()\n",
        "    jitted_apply(params, x)\n",
        "    end_time = time.time()\n",
        "    total_time += end_time - start_time\n",
        "  average_attention_time = total_time / num_iterations\n",
        "  print(f\"Average inference time: {average_attention_time} seconds\")\n",
        "  return average_attention_time\n"
      ],
      "metadata": {
        "id": "eTJNqKlhB3-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestMultiHeadEinsum(unittest.TestCase):\n",
        "  def test_multihead_einsum(self):\n",
        "    head_dim = 4\n",
        "    num_heads = 2\n",
        "    batch_size = 1\n",
        "    seq_length = 2\n",
        "    hidden_dim = head_dim * num_heads\n",
        "    random_key = jax.random.PRNGKey(0)\n",
        "\n",
        "    new_model = MultiHeadAttentionTask3Solution(\n",
        "        head_dim=head_dim,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "\n",
        "    new_params = new_model.init(random_key, jnp.ones((batch_size, seq_length, hidden_dim), dtype=jnp.int16))\n",
        "\n",
        "    baseline_model = MultiHeadAttentionReferenceTask3(\n",
        "        head_dim=head_dim,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "    baseline_params = baseline_model.init(random_key, jnp.ones((batch_size, seq_length, hidden_dim), dtype=jnp.int16))\n",
        "\n",
        "    baseline_params['params']['heads_0']['query']['kernel'] = new_params['params']['query']['kernel'][:, :4].copy()\n",
        "    baseline_params['params']['heads_0']['key']['kernel'] = new_params['params']['key']['kernel'][:, :4].copy()\n",
        "    baseline_params['params']['heads_0']['value']['kernel'] = new_params['params']['value']['kernel'][:, :4].copy()\n",
        "    baseline_params['params']['heads_1']['query']['kernel'] = new_params['params']['query']['kernel'][:, 4:].copy()\n",
        "    baseline_params['params']['heads_1']['key']['kernel'] = new_params['params']['key']['kernel'][:, 4:].copy()\n",
        "    baseline_params['params']['heads_1']['value']['kernel'] = new_params['params']['value']['kernel'][:, 4:].copy()\n",
        "    baseline_params['params']['dense']['kernel'] = new_params['params']['dense']['kernel'].copy()\n",
        "    baseline_params['params']['dense']['bias'] = new_params['params']['dense']['bias'].copy()\n",
        "\n",
        "    baseline_res = baseline_model.apply(baseline_params, jnp.ones((batch_size, seq_length, hidden_dim), dtype=jnp.int16))\n",
        "    new_res = new_model.apply(new_params, jnp.ones((batch_size, seq_length, hidden_dim), dtype=jnp.int16))\n",
        "    self.assertTrue(np.allclose(new_res, baseline_res))\n",
        "\n",
        "TestMultiHeadEinsum().test_multihead_einsum()"
      ],
      "metadata": {
        "id": "LuyNnmY-ybXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSmAJzKZyIf1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}